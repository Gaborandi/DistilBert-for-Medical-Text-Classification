{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5707db16",
   "metadata": {},
   "source": [
    "Fine Tuning DistilBert for MultiClass Text Classification\n",
    "\n",
    "==> Flow of the notebook: \n",
    "\n",
    "- Importing Python Libraries and preparing the environment\n",
    "- Importing and Pre-Processing the domain data\n",
    "- Preparing the Dataset and Dataloader\n",
    "- Creating the Neural Network for Fine Tuning\n",
    "- Fine Tuning the Model\n",
    "- Validating the Model Performance\n",
    "- Saving the model and artifacts for Inference in Future\n",
    "\n",
    "\n",
    "\n",
    "==> Language Model Used:\n",
    "\n",
    "- DistilBERT this is a smaller transformer model as compared to BERT or Roberta. It is created by process of distillation applied to Bert.\n",
    "- Research Paper : (https://arxiv.org/abs/1910.01108)\n",
    "- HuggingFace Documentation for python : (https://huggingface.co/transformers/model_doc/distilbert.html)\n",
    "\n",
    "==> Script Objective:\n",
    "\n",
    "- The objective of this script is to fine tune DistilBERT to be able to do MeSH Classification (Hepatitis)\n",
    "\n",
    "==> Importing Python Libraries and preparing the environment :\n",
    "\n",
    "- Pandas , Numpy\n",
    "- Pytorch and Pytorch Utils for Dataset and Dataloader\n",
    "- Transformers\n",
    "- DistilBERT Model and Tokenizer\n",
    "- sklearn for metrics (to caLculate the hamming loss and the hamming score)\n",
    "- warnings\n",
    "- tqdm \n",
    "- logging\n",
    "\n",
    "- Followed by that we will preapre the device for CUDA execeution. This configuration is needed if you want to leverage on onboard GPU but you can use CPU also (you will graduate before finishing the training :D)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3901bc25",
   "metadata": {},
   "source": [
    "==> Very Important Note : \n",
    "\n",
    "=>the overall mechanisms for a multiclass and multilabel problems are similar, except two major points:\n",
    "  \n",
    "- Loss function is designed to evaluate all the probability of categories individually rather than as compared to other         \n",
    "  categories. Hence the use of Binary Cross Entropy \"BCE\" rather than Cross Entropy when defining loss:       \n",
    "  1) https://medium.com/dejunhuang/learning-day-57-practical-5-loss-function-crossentropyloss-vs-bceloss-in-pytorch-softmax-vs-bd866c8a0d23\n",
    "  \n",
    "  \n",
    "- Sigmoid of the outputs calcuated to rather than Softmax. so, we use The loss metrics and Hamming Score for direct comparison     of expected vs predicted: \n",
    "  1) https://towardsdatascience.com/sigmoid-activation-and-binary-crossentropy-a-less-than-perfect-match-b801e130e31\n",
    "  \n",
    "  2) https://www.linkedin.com/pulse/hamming-score-multi-label-classification-chandra-sharat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a141a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed libraries\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f7fee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdfc6d4",
   "metadata": {},
   "source": [
    "==> Importing and Pre-Processing the domain data\n",
    "\n",
    "- Import the file in a dataframe \n",
    "- remove the pmid column from the data.\n",
    "- A new dataframe is made and input text is stored in the text column.\n",
    "- The values of all the categories converted into a list.\n",
    "- The list is appened as a new column names as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba91caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('pubmed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4819e398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>txt</th>\n",
       "      <th>label_cir</th>\n",
       "      <th>label_nfl</th>\n",
       "      <th>label_hep</th>\n",
       "      <th>label_hpc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30558055</td>\n",
       "      <td>abo incompatible living donor liver transplant...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30558011</td>\n",
       "      <td>a human ciliopathy with polycystic ovarian syn...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30540737</td>\n",
       "      <td>vibrio cholerae no - o1 no - o139 bacteremia i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30531115</td>\n",
       "      <td>ruptured ascending colonic varices in a patien...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30526986</td>\n",
       "      <td>a 44 - year - old woman with sudden breathless...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30507656</td>\n",
       "      <td>short article sequence variations of pkhd1 und...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30508897</td>\n",
       "      <td>primary biliary cirrhosis with refractory hypo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30482030</td>\n",
       "      <td>auxiliary partial orthotopic liver transplanta...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30471833</td>\n",
       "      <td>intraoperative management of a patient with im...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30464164</td>\n",
       "      <td>successful treatment of repeated hematemesis s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pmid                                                txt  label_cir  \\\n",
       "0  30558055  abo incompatible living donor liver transplant...          1   \n",
       "1  30558011  a human ciliopathy with polycystic ovarian syn...          1   \n",
       "2  30540737  vibrio cholerae no - o1 no - o139 bacteremia i...          1   \n",
       "3  30531115  ruptured ascending colonic varices in a patien...          1   \n",
       "4  30526986  a 44 - year - old woman with sudden breathless...          1   \n",
       "5  30507656  short article sequence variations of pkhd1 und...          1   \n",
       "6  30508897  primary biliary cirrhosis with refractory hypo...          1   \n",
       "7  30482030  auxiliary partial orthotopic liver transplanta...          1   \n",
       "8  30471833  intraoperative management of a patient with im...          1   \n",
       "9  30464164  successful treatment of repeated hematemesis s...          1   \n",
       "\n",
       "   label_nfl  label_hep  label_hpc  \n",
       "0          0          0          0  \n",
       "1          0          0          0  \n",
       "2          0          0          0  \n",
       "3          0          0          0  \n",
       "4          0          0          0  \n",
       "5          0          0          0  \n",
       "6          0          0          0  \n",
       "7          0          0          0  \n",
       "8          0          0          0  \n",
       "9          0          0          0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68ddf1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['pmid'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "056f6024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>label_cir</th>\n",
       "      <th>label_nfl</th>\n",
       "      <th>label_hep</th>\n",
       "      <th>label_hpc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abo incompatible living donor liver transplant...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a human ciliopathy with polycystic ovarian syn...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vibrio cholerae no - o1 no - o139 bacteremia i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ruptured ascending colonic varices in a patien...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a 44 - year - old woman with sudden breathless...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>short article sequence variations of pkhd1 und...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>primary biliary cirrhosis with refractory hypo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>auxiliary partial orthotopic liver transplanta...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>intraoperative management of a patient with im...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>successful treatment of repeated hematemesis s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 txt  label_cir  label_nfl  \\\n",
       "0  abo incompatible living donor liver transplant...          1          0   \n",
       "1  a human ciliopathy with polycystic ovarian syn...          1          0   \n",
       "2  vibrio cholerae no - o1 no - o139 bacteremia i...          1          0   \n",
       "3  ruptured ascending colonic varices in a patien...          1          0   \n",
       "4  a 44 - year - old woman with sudden breathless...          1          0   \n",
       "5  short article sequence variations of pkhd1 und...          1          0   \n",
       "6  primary biliary cirrhosis with refractory hypo...          1          0   \n",
       "7  auxiliary partial orthotopic liver transplanta...          1          0   \n",
       "8  intraoperative management of a patient with im...          1          0   \n",
       "9  successful treatment of repeated hematemesis s...          1          0   \n",
       "\n",
       "   label_hep  label_hpc  \n",
       "0          0          0  \n",
       "1          0          0  \n",
       "2          0          0  \n",
       "3          0          0  \n",
       "4          0          0  \n",
       "5          0          0  \n",
       "6          0          0  \n",
       "7          0          0  \n",
       "8          0          0  \n",
       "9          0          0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fbeb84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "new_df['text'] = data['txt']\n",
    "new_df['labels'] = data.iloc[:, 1:].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4320270a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abo incompatible living donor liver transplant...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a human ciliopathy with polycystic ovarian syn...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vibrio cholerae no - o1 no - o139 bacteremia i...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ruptured ascending colonic varices in a patien...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a 44 - year - old woman with sudden breathless...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>short article sequence variations of pkhd1 und...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>primary biliary cirrhosis with refractory hypo...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>auxiliary partial orthotopic liver transplanta...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>intraoperative management of a patient with im...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>successful treatment of repeated hematemesis s...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        labels\n",
       "0  abo incompatible living donor liver transplant...  [1, 0, 0, 0]\n",
       "1  a human ciliopathy with polycystic ovarian syn...  [1, 0, 0, 0]\n",
       "2  vibrio cholerae no - o1 no - o139 bacteremia i...  [1, 0, 0, 0]\n",
       "3  ruptured ascending colonic varices in a patien...  [1, 0, 0, 0]\n",
       "4  a 44 - year - old woman with sudden breathless...  [1, 0, 0, 0]\n",
       "5  short article sequence variations of pkhd1 und...  [1, 0, 0, 0]\n",
       "6  primary biliary cirrhosis with refractory hypo...  [1, 0, 0, 0]\n",
       "7  auxiliary partial orthotopic liver transplanta...  [1, 0, 0, 0]\n",
       "8  intraoperative management of a patient with im...  [1, 0, 0, 0]\n",
       "9  successful treatment of repeated hematemesis s...  [1, 0, 0, 0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9622b1a",
   "metadata": {},
   "source": [
    "==> Preparing the Dataset and Dataloader\n",
    "\n",
    "- We will start by defining few key variables that will be used later during the training/fine tuning stage. \n",
    "- Followed by creation of MultiLabelHepatitis class : This defines how the text is pre-processed before sending it to the neural network. \n",
    "- We will also define the Dataloader that will feed the data in batches to the neural network for suitable training and     \n",
    "  processing. Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing   and its passage to neural network. For further reading into Dataset and Dataloader read the docs at PyTorch\n",
    "\n",
    "\n",
    "==> MultiLabelHepatitis Dataset Class\n",
    "\n",
    "- This class is defined to accept the tokenizer, dataframe and max_length as input and generate tokenized output and tags that     is used by the DistilBERT model for training.\n",
    "- We are using the DistilBERT tokenizer to tokenize the data in the text column of the dataframe.\n",
    "- The tokenizer uses the encode_plus method to perform tokenization and generate the necessary outputs, namely: \n",
    "  1)ids  2)attention_mask  3)token_type_ids\n",
    "- targets is the list of categories labled as 0 or 1 in the dataframe.\n",
    "- this class is used to create 2 datasets, for training and for validation.\n",
    "- Training Dataset : we use 80% of the PubMed data for the fine tunining \n",
    "- Validation Dataset : is used to evaluate the performance of the model.\n",
    "\n",
    "==> Dataloader\n",
    "\n",
    "- Dataloader is used for creating training and validation dataloader that load data to the neural network in a defined manner   because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and   then passed to the neural network needs to be controlled.\n",
    "- This control is achieved by using the parameters such as batch_size and max_len.\n",
    "- Training and Validation dataloaders are used in the training and validation part of the flow respectively\n",
    "- for further reading about Dataloader : https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09351d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128\n",
    "\n",
    "# The number of training samples that will be propagated through the neural network\n",
    "TRAIN_BATCH_SIZE = 4 \n",
    "\n",
    "# The number of validation samples that will be propagated through the neural network\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "# How many times the entire dataset is passed forward and backward through the neural network\n",
    "EPOCHS = 6\n",
    "\n",
    "# A hyperparameter that controls how much to change the model in response to the estimated error each time the model weights \n",
    "# are updated (a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving \n",
    "# toward a minimum of a loss function).\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f1b9545",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f3f4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelHepatits(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask'] \n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9c2d6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (6726, 2)\n",
      "TRAIN Dataset: (5381, 2)\n",
      "TEST Dataset: (1345, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.8\n",
    "train_data=new_df.sample(frac=train_size,random_state=200)\n",
    "test_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = MultiLabelHepatits(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = MultiLabelHepatits(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9759d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cacf90e",
   "metadata": {},
   "source": [
    "==> Creating the Neural Network for Fine Tuning\n",
    "\n",
    "==> Neural Network:\n",
    "\n",
    "- We will be creating a neural network with the DistillBERTClass.\n",
    "- This network will have the DistilBERT Language model followed by a dropout (for Regularization) and finally a Linear layer (for Classification) to obtain the final outputs.\n",
    "- The data will be fed to the DistilBERT Language model as defined in the dataset.\n",
    "- Final layer outputs is what will be compared to the encoded category to determine the accuracy of models prediction.\n",
    "- The number of dimensions for Linear Layer is \"2\" because that is the total number of categories in which we are looking to classify our model (number of categories in the labels column).\n",
    "- We will initiate an instance of the network called model. This instance will be used for training and then to save the final trained model for future inference.\n",
    "\n",
    "==> Loss Function and Optimizer:\n",
    "\n",
    "- loss_fn : is the Loss Function that is used the calculate the difference in the output created by the model and the actual                 output. the loss function used will be a combination of Binary Cross Entropy which is implemented as BCELogits Loss             in PyTorch\n",
    "- Optimizer : is used to update the weights of the neural network to improve its performance.\n",
    "\n",
    "==> Further Reading:\n",
    "\n",
    "- Pytorch Tutorials to get an intuition of Loss Function and Optimizer : (https://github.com/abhimishra91/pytorch-tutorials)\n",
    "- Pytorch Tutorials for BCE Loss : https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss\n",
    "- Pytorch Documentation for Loss Function : (https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "- Pytorch Documentation for Optimizer : (https://pytorch.org/docs/stable/optim.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f51c1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class DistilBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistilBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 256) #reduce the dim instead of 768 ==> 256\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier = torch.nn.Linear(256, 4) #modified\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "model = DistilBERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a525c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec7bc068",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee69192",
   "metadata": {},
   "source": [
    "==> Fine Tuning the Model\n",
    "\n",
    "- Here we define a training function that trains the model on the training dataset created above \n",
    "\n",
    "==> Fine Tunining the Neural Network\n",
    "\n",
    "- The dataloader passes data to the model based on the batch size (batch size to lead to some errors, so try to choose a number wisely).\n",
    "- Subsequent output from the model and the actual category are compared to calculate the loss.\n",
    "- Loss value is used to optimize the weights of the neurons in the network.\n",
    "- After every 5000 (not a constant number) steps the loss value is printed in the console.\n",
    "- As you can see just in \"3\" epochs by the final step the model was working with a miniscule loss of 0.012 i.e. the network output is extremely close to the actual output, \n",
    "- note : in the training output, number of epochs similar to the python indexing, starts from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29b46713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12dfe94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "1it [00:00,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7057749032974243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1346it [08:18,  2.70it/s]\n",
      "1it [00:00,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  0.04012896865606308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1346it [08:19,  2.70it/s]\n",
      "1it [00:00,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss:  0.02784394472837448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1346it [08:19,  2.69it/s]\n",
      "1it [00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss:  0.012844439595937729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1346it [08:19,  2.69it/s]\n",
      "1it [00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss:  0.1364152580499649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1346it [08:19,  2.69it/s]\n",
      "1it [00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss:  0.003160130698233843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1346it [08:20,  2.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76682ddf",
   "metadata": {},
   "source": [
    "==> Validating the Model\n",
    "\n",
    "- in this step, we pass the Testing Dataset to the model. This step tells us how good the model performs on this unseen data.\n",
    "- This data is the 20% of 'pubmed.csv' which was seperated during the Dataset creation stage. \n",
    "- During the validation stage the weights of the model are not updated. Only the final output is compared to the actual value,     This comparison is then used to calcuate the accuracy of the model.\n",
    "- During the validation stage we pass the unseen data(Testing Dataset) to the model. This step determines how good the model       performs on the unseen data.\n",
    "\n",
    "\n",
    "- To our models performance we are using the following metrics:  \n",
    "  1)Hamming Score : is the fraction of correct predictions compared to the total labels (Accuracy : the overall                       percentage of predictions without errors)   \n",
    "  2)Hamming Loss : is a good measure of model performance. lower the Hamming loss better the model performance which here equals to '0.036'\n",
    "- for further reading: https://www.linkedin.com/pulse/hamming-score-multi-label-classification-chandra-sharat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b248f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataValidation(testing_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b8055be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "337it [00:40,  8.42it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs, targets = DataValidation(testing_loader)\n",
    "final_outputs = np.array(outputs) >=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b044cb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Score= 0.9066914498141264\n",
      "Hamming Loss = 0.03828996282527881\n"
     ]
    }
   ],
   "source": [
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)\n",
    "\n",
    "val_hamming_loss  = metrics.hamming_loss(targets, final_outputs)\n",
    "val_hamming_score = hamming_score(np.array(targets), np.array(final_outputs))\n",
    "\n",
    "print(f\"Hamming Score= {val_hamming_score}\")\n",
    "print(f\"Hamming Loss = {val_hamming_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d3411",
   "metadata": {},
   "source": [
    "==> Saving the Trained Model for inference\n",
    "\n",
    "- This is the final step in the process of fine tuning the model.\n",
    "- The model and its vocabulary are saved in your local path. \n",
    "- These files are then used in the future to make inference on new inputs of news headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e103f013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Saving the files for inference\n",
    "\n",
    "output_model_file = 'pytorch_distilbert_pubmed.bin'\n",
    "output_vocab_file = 'vocabulary_distilbert_.bin'\n",
    "\n",
    "torch.save(model, output_model_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)\n",
    "\n",
    "print('Saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
